{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065a328c-bbd0-4c0a-8823-4893edb5b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read input.txt which contains shakespeare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976294e3-9657-4d1b-9a27-18c307c30b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000d16aa-419b-47f0-ad80-6e93356c8109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#working with 1M characters\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e56070e6-2454-43e0-adfc-6b25c2b31ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "#first 100 characters in the text file\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73743753-ce0c-4958-b6dc-72b749af7afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac50e379-738e-4aa4-8044-9a2158c95bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#character level tokenizer\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for i, s in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81279eae-36ef-4888-a0a0-5c0ab3c0a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 2]\n",
      "hello!\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hello!\"))\n",
    "print(decode(encode(\"hello!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639c59cf-2288-4601-a864-0472b2b63641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb46c21-3cd5-4c87-8ee2-6b7d60aa611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "498f4403-67a9-4d01-a93d-f4e1ac08aeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F followed by i\n",
      "Fi followed by r\n",
      "Fir followed by s\n",
      "Firs followed by t\n",
      "First followed by  \n",
      "First  followed by C\n",
      "First C followed by i\n",
      "First Ci followed by t\n"
     ]
    }
   ],
   "source": [
    "#training data pairs\n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(decode((x[:t+1]).tolist()), \"followed by\", decode([(y[t]).item()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f94278-0040-4505-aeb6-ba438a6a832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "target torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # of independent sequences to process in parallel\n",
    "block_size = 8 #maximum context len for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  #random indexes to pick block_size num of characters\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs', xb.shape)\n",
    "print(xb)\n",
    "print('target', yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d163bb-4e59-4521-8fe1-9c79610770b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input L output e\n",
      "input Le output t\n",
      "input Let output '\n",
      "input Let' output s\n",
      "input Let's output  \n",
      "input Let's  output h\n",
      "input Let's h output e\n",
      "input Let's he output a\n",
      "input f output o\n",
      "input fo output r\n",
      "input for output  \n",
      "input for  output t\n",
      "input for t output h\n",
      "input for th output a\n",
      "input for tha output t\n",
      "input for that output  \n",
      "input n output t\n",
      "input nt output  \n",
      "input nt  output t\n",
      "input nt t output h\n",
      "input nt th output a\n",
      "input nt tha output t\n",
      "input nt that output  \n",
      "input nt that  output h\n",
      "input M output E\n",
      "input ME output O\n",
      "input MEO output :\n",
      "input MEO: output \n",
      "\n",
      "input MEO:\n",
      " output I\n",
      "input MEO:\n",
      "I output  \n",
      "input MEO:\n",
      "I  output p\n",
      "input MEO:\n",
      "I p output a\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(\"input\", decode(context.tolist()), \"output\", decode([target.item()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f42c93-2779-44e2-8118-fdf0030de4b7",
   "metadata": {},
   "source": [
    "**Starting with the bigram language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7cb7674-8ed9-4869-b11d-c4fc6a797e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "4.878634929656982\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        #nn.Embedding(vocab_size, size of the vector representing each token)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) #B(batch), T(token_size), C(size of 1D vector representing each tozen)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B, T) is array of indicies in the current context \n",
    "        for _ in range(max_new_tokens):\n",
    "            #prediction\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on last time step\n",
    "            logits = logits[:, -1, :] #becomes (B, C)\n",
    "            #apply softmax to get probabilites\n",
    "            probs = F.softmax(logits, dim = -1) #dim = (B, C)\n",
    "            #sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B, 1)\n",
    "            #append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m(xb, yb)\n",
    "print(out.shape)\n",
    "print(loss.item())\n",
    "\n",
    "\n",
    "#zero represents uniline character\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "739b2da0-4017-445a-9829-e1b7397340f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) #previously only used SGD(stochastic gradient descent, here using adam)\n",
    "#adam will adapt the learning rate unlike SGD which has a constant learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6323b9a-4beb-4a78-a773-15434308d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4832067489624023\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(20000):\n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23d4d1f5-f884-40f3-9ee2-f28ea2d907cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "QUCatheesicis bas, angomitouze chifomerawhe ty ag\n",
      "\n",
      "Doone thesellande west, me fffol, whandiffe IAfithods misue, knild he I:\n",
      "Whe! toudirer' My ayosbly louroura s m', uthos s reveprthoukerdi't avorure fotemowe.\n",
      "Whamo es t, tstt g t RTRushy,\n",
      "WAsbr spr my ou pl y,\n",
      "Witoft at o s me,\n",
      "Whabr'the Cicuoman\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype = torch.long), max_new_tokens = 300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e8333-5095-4ccc-a527-e152530aaf9d",
   "metadata": {},
   "source": [
    "**SELF ATTENTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbee7eb-6a3f-42ef-86ba-301a4d06178c",
   "metadata": {},
   "source": [
    "Self Attention Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ded177a7-9348-4138-a02e-ed0cc6442cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#toy example\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2015f5a8-1580-45e9-88f4-0dfabbe93b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C)) #xbow is bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc5a5a5-4287-49d8-89b7-af272021dedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d4bd936-ecea-4933-94e0-cb28216a300a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0] #row 2 is sum of row 1 and row 2, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "763a5072-62b4-4a3d-abdf-1ccd63b61e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57b40499-0307-4693-945d-79169837b619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a/torch.sum(a, 1, keepdim = True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32b4196c-b175-43ae-be73-72e2c00223ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei torch.Size([8, 8])\n",
      "x torch.Size([4, 8, 2])\n",
      "xbow2 torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "#easier way to achieve xbow \n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "print('wei', wei.shape)\n",
    "print('x', x.shape)\n",
    "wei = wei/torch.sum(wei, 1, keepdim = True)\n",
    "xbow2 = wei @ x\n",
    "print('xbow2', xbow2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66bbf9f5-35b5-477f-8d44-819bacb08974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wei torch.Size([8, 8])\n",
      "xbox3 torch.Size([4, 8, 2])\n"
     ]
    }
   ],
   "source": [
    "#third version using softmax\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print('wei', wei.shape)\n",
    "xbow3 = wei @ x\n",
    "print('xbox3', xbow3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2383e275-740f-41b6-9f0a-6b1cdf4c3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the goal of this is to have 2nd token to be average of all tokens upto 2, 3rd token to be the average of all tokens upto 3, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9cce203-70e0-46e8-ab32-17a06c41d76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#complete single head self-attention operations\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn((B, T, C)) #x data \n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias = False)\n",
    "query = nn.Linear(C, head_size, bias = False)\n",
    "value = nn.Linear(C, head_size, bias = False)\n",
    "k = key(x) # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei = q@k.transpose(-2, -1) #(B, T, 16) @ (B, 16, T) --. (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei/head_size**-0.5 #important to keep variance at 1 to prevent wei from becoming like one-hot encoding\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db45b7-775b-4c39-aa9e-caad3497c086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLSideProject (GPU)",
   "language": "python",
   "name": "dlsideproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
